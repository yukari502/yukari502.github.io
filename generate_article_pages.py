#!/usr/bin/env python3
"""
Generate static HTML pages for each article.
This enables better SEO and direct article linking while maintaining the SPA experience for the main site.
"""

import os
import json
import re
import urllib.parse
import glob
import html
import hashlib
from pathlib import Path

def read_markdown_file(file_path):
    """Read markdown file and strip frontmatter"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Strip frontmatter if present
    content = re.sub(r'^---\s*\n[\s\S]*?\n---\s*\n', '', content)
    return content

def calculate_hash(content_list):
    """Calculate MD5 hash of a list of strings"""
    hasher = hashlib.md5()
    for content in content_list:
        hasher.update(content.encode('utf-8'))
    return hasher.hexdigest()

def get_stored_hash(file_path):
    """Extract stored hash from HTML file meta tag"""
    if not os.path.exists(file_path):
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            match = re.search(r'<meta name="build-hash" content="([a-f0-9]+)">', content)
            if match:
                return match.group(1)
    except Exception:
        pass
    return None

def generate_article_html(article, template_content, base_url, output_dir):
    """Generate HTML file for a single article"""
    
    # Read the markdown file
    file_path = urllib.parse.unquote(article['path'])
    
    if not os.path.exists(file_path):
        print(f"Warning: File not found: {file_path}")
        return None
    
    markdown_content = read_markdown_file(file_path)
    
    # Use slug from article info (generated by generate_articles_json.py)
    # If not present (backward compatibility), fallback to simple slugify
    if 'slug' in article:
        slug = article['slug']
    else:
        # Fallback slugify if not in json
        slug = article['filename'].replace('.md', '').lower()
        slug = re.sub(r'[^\w\s-]', '', slug)
        slug = re.sub(r'[-\s]+', '-', slug).strip('-')
        
    if 'category' in article:
        pass

    # Actually, we can just parse the article['url'] if available
    if 'url' in article:
        # url is like /posts/category/slug.html
        # Remove leading /
        rel_url = article['url'].lstrip('/')
        output_file = rel_url
    else:
        # Fallback logic
        category_slug = article.get('category', 'uncategorized').lower().replace(' ', '-')
        output_file = os.path.join('posts', category_slug, f"{slug}.html")

    # Calculate new hash (Markdown Content + Template Content + Title + Date)
    # Including metadata ensures changes in frontmatter also trigger rebuild
    current_hash = calculate_hash([
        markdown_content, 
        template_content, 
        article.get('title', ''),
        article.get('date', ''),
        article.get('description', '')
    ])
    
    # Check if rebuild is needed
    stored_hash = get_stored_hash(output_file)
    if stored_hash == current_hash:
        # print(f"Skipped (No Change): {output_file}") # Optional: reduce noise
        return article.get('url', f"/posts/{slug}.html")

    # Ensure directory exists
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # Prepare template variables
    # Use the pre-generated URL or construct it
    output_url = article.get('url', f"/posts/{slug}.html") # This might be wrong in fallback but we expect url to be there
    
    # Calculate relative path to root for resources (script.js, style.css, Pic/)
    article_output_dir = os.path.dirname(output_file)
    relative_root = os.path.relpath('.', article_output_dir)
    
    if relative_root == '.':
        relative_root = ''
    else:
        relative_root += '/'

    replacements = {
        '{TITLE}': html.escape(article['title']),
        '{DESCRIPTION}': html.escape(article.get('description', '')[:160]),  # Limit for meta description
        '{KEYWORDS}': html.escape(f"{article.get('category', '')}, {article['title']}"),
        '{URL}': f"{base_url}{output_url}",
        '{DATE}': article.get('date', ''),
        '{CATEGORY}': article.get('category', 'Uncategorized'),
        '{CONTENT}': markdown_content.replace('</script>', '<\\/script>'),  # Prevent script injection
        '{SLUG}': html.escape(slug),
        '{ROOT_PATH}': relative_root
    }
    
    # Replace placeholders in template
    html_content = template_content
    for key, value in replacements.items():
        html_content = html_content.replace(key, str(value))
    
    # Inject Build Hash Meta Tag
    # We insert it before </head>
    hash_meta = f'<meta name="build-hash" content="{current_hash}">'
    if '</head>' in html_content:
        html_content = html_content.replace('</head>', f'    {hash_meta}\n</head>')
    else:
        # Fallback if no head tag (unlikely)
        html_content += f"\n<!-- Build Hash: {current_hash} -->"

    # Write the HTML file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"Generated: {output_file}")
    
    # Return the URL path for sitemap
    return output_url

def generate_all_articles():
    """Generate HTML pages for all articles"""
    
    articles_json = 'articles.json'
    template_file = 'article-template.html'
    output_dir = 'posts'
    base_url = 'https://yukari502.github.io'
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Read template
    if not os.path.exists(template_file):
        print(f"Error: Template file not found: {template_file}")
        return []
    
    with open(template_file, 'r', encoding='utf-8') as f:
        template_content = f.read()
    
    # Read articles
    if not os.path.exists(articles_json):
        print(f"Error: Articles JSON not found: {articles_json}")
        return []
    
    with open(articles_json, 'r', encoding='utf-8') as f:
        articles = json.load(f)
    
    # Generate HTML for each article
    generated_urls = []
    generated_count = 0
    skipped_count = 0
    
    for article in articles:
        # Check if file was actually generated (we can infer from logs, but here we just count)
        # To count accurately, we'd need generate_article_html to return a status.
        # But for now, let's just run it.
        
        # We can check modification time or just trust the function.
        # Let's modify generate_article_html slightly to return status? 
        # No, let's just keep it simple.
        
        url_path = generate_article_html(article, template_content, base_url, output_dir)
        if url_path:
            generated_urls.append({
                'url': url_path,
                'lastmod': article.get('date', ''),
                'title': article.get('title', '')
            })
            
    print(f"\nâœ… Processed {len(generated_urls)} articles.")
    
    return generated_urls

if __name__ == "__main__":
    generate_all_articles()
